<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-GPU Training: Hardware Topology Analysis | A.H. Javid</title>
    <meta name="description" content="Comprehensive analysis revealing why more GPUs doesn't always mean better performance. Hardware topology considerations for distributed training.">
    <link rel="stylesheet" href="../../assets/css/post.css">
</head>
<body>
    <nav class="site-nav">
        <div class="nav-container">
            <a href="../../" class="nav-brand">A.H. Javid</a>
            <div class="nav-links">
                <a href="../../#research">Research</a>
                <a href="https://github.com/ahjavid/technical-notes-blog">GitHub</a>
            </div>
        </div>
    </nav>

    <header class="article-header">
        <div class="article-header-inner">
            <a href="../../" class="back-link">← Back to Research</a>
            <span class="article-category">GPU Architecture</span>
            <h1 class="article-title">Multi-GPU Training: When Hardware Topology Matters</h1>
            <p class="article-subtitle">
                Why more GPUs doesn't always mean better performance. 120+ hours of testing with 
                dual RTX 4070 Ti SUPER reveals critical hardware topology considerations for 
                distributed training decisions.
            </p>
            <div class="article-meta">
                <span class="article-meta-item">June 2025</span>
                <span class="article-meta-item">25 min read</span>
                <span class="article-meta-item">120+ Hours Testing</span>
            </div>
        </div>
    </header>

    <article class="article-content">
        <div class="key-findings">
            <div class="key-findings-title">Key Findings</div>
            <ul>
                <li><strong>Parameter threshold:</strong> Models under 10M parameters perform worse on multi-GPU</li>
                <li><strong>Hardware topology impact:</strong> PCIe Host Bridge prevents P2P GPU communication</li>
                <li><strong>Production insights:</strong> Cost-benefit analysis reveals negative ROI scenarios</li>
                <li><strong>Decision framework:</strong> Automated selection for optimal GPU resource allocation</li>
            </ul>
        </div>

        <h2>The Counterintuitive Result</h2>
        <p>
            Conventional wisdom suggests that more GPUs always lead to faster training. Our 
            rigorous testing revealed this assumption can be dangerously wrong—especially for 
            consumer-grade hardware configurations commonly found in research labs and startups.
        </p>

        <div class="callout callout-error">
            <div class="callout-title">Critical Finding</div>
            <div class="callout-content">
                <p>
                    Models with fewer than 10M parameters trained <strong>slower</strong> on 2 GPUs 
                    compared to a single GPU in our PCIe topology. Communication overhead exceeded 
                    any parallelization benefits.
                </p>
            </div>
        </div>

        <h2>Hardware Configuration</h2>
        
        <div class="stats-row">
            <div class="stat-item">
                <div class="stat-value">2×</div>
                <div class="stat-label">RTX 4070 Ti SUPER</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">16GB</div>
                <div class="stat-label">VRAM Each</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">PCIe 4.0</div>
                <div class="stat-label">×16 Slots</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">Host Bridge</div>
                <div class="stat-label">No P2P Access</div>
            </div>
        </div>

        <h3>The Topology Problem</h3>
        <p>
            Our GPUs connect through a PCIe Host Bridge rather than NVLink or direct P2P. 
            This means all inter-GPU communication must traverse the CPU, adding significant 
            latency to gradient synchronization.
        </p>

        <div class="code-block">
            <div class="code-block-header">
                <span>nvidia-smi topo -m</span>
            </div>
            <pre><code>        GPU0    GPU1    CPU
GPU0     X      PHB     SYS
GPU1    PHB      X      SYS
CPU     SYS     SYS      X

Legend:
  PHB = PCIe Host Bridge (all communication goes through CPU)
  SYS = System/CPU interconnect</code></pre>
        </div>

        <h2>Benchmark Results</h2>

        <h3>Training Time by Model Size</h3>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Parameters</th>
                        <th>Single GPU</th>
                        <th>Dual GPU</th>
                        <th>Speedup</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Small CNN</td>
                        <td>1.2M</td>
                        <td>45 min</td>
                        <td>68 min</td>
                        <td class="value-negative">0.66×</td>
                    </tr>
                    <tr>
                        <td>MobileNetV2</td>
                        <td>3.5M</td>
                        <td>1.2 hrs</td>
                        <td>1.5 hrs</td>
                        <td class="value-negative">0.80×</td>
                    </tr>
                    <tr>
                        <td>ResNet-50</td>
                        <td>25M</td>
                        <td>4.5 hrs</td>
                        <td>3.8 hrs</td>
                        <td class="value-neutral">1.18×</td>
                    </tr>
                    <tr>
                        <td>ResNet-152</td>
                        <td>60M</td>
                        <td>12 hrs</td>
                        <td>7.5 hrs</td>
                        <td class="value-positive">1.60×</td>
                    </tr>
                    <tr>
                        <td>ViT-Large</td>
                        <td>307M</td>
                        <td>48 hrs</td>
                        <td>28 hrs</td>
                        <td class="value-positive">1.71×</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h3>The 10M Parameter Threshold</h3>
        <p>
            We identified approximately 10M parameters as the crossover point where multi-GPU 
            training begins to provide benefits in our topology. Below this threshold, 
            single-GPU training is consistently faster.
        </p>

        <div class="callout callout-info">
            <div class="callout-title">Communication vs Computation</div>
            <div class="callout-content">
                <p>
                    The ratio of gradient communication time to forward/backward pass time 
                    determines multi-GPU efficiency:
                </p>
                <ul>
                    <li><strong>&lt;10M params:</strong> Communication dominates (30-50% of step time)</li>
                    <li><strong>10-50M params:</strong> Balanced regime (15-30%)</li>
                    <li><strong>&gt;50M params:</strong> Computation dominates (5-15%)</li>
                </ul>
            </div>
        </div>

        <h2>Cost-Benefit Analysis</h2>

        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Time Saved</th>
                        <th>Extra Power Cost</th>
                        <th>3-Month ROI</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Small models (&lt;10M)</td>
                        <td class="value-negative">-50%</td>
                        <td>+$45</td>
                        <td class="value-negative">-$180</td>
                    </tr>
                    <tr>
                        <td>Medium models (10-50M)</td>
                        <td>+20%</td>
                        <td>+$45</td>
                        <td class="value-neutral">+$35</td>
                    </tr>
                    <tr>
                        <td>Large models (&gt;50M)</td>
                        <td class="value-positive">+70%</td>
                        <td>+$45</td>
                        <td class="value-positive">+$420</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h2>Decision Framework</h2>
        
        <p>
            Based on our findings, we developed an automated decision framework for GPU 
            resource allocation:
        </p>

        <div class="code-block">
            <div class="code-block-header">
                <span>Python</span>
            </div>
            <pre><code>def select_gpu_strategy(model, topology):
    """Select optimal GPU strategy based on model and hardware."""
    params = count_parameters(model)
    has_p2p = topology.supports_p2p()
    
    if params < 10_000_000 and not has_p2p:
        return "single_gpu"
    elif params < 50_000_000:
        return "data_parallel" if has_p2p else "single_gpu"
    else:
        return "data_parallel"  # Always beneficial for large models</code></pre>
        </div>

        <h2>Recommendations</h2>

        <div class="cards-grid">
            <div class="card">
                <div class="card-title">For Small Models</div>
                <div class="card-content">
                    Use single GPU training. Multi-GPU overhead exceeds benefits. 
                    Focus on batch size optimization instead.
                </div>
            </div>
            <div class="card">
                <div class="card-title">For Medium Models</div>
                <div class="card-content">
                    Test both configurations. Results vary by specific architecture 
                    and gradient complexity.
                </div>
            </div>
            <div class="card">
                <div class="card-title">For Large Models</div>
                <div class="card-content">
                    Multi-GPU training provides clear benefits. Consider gradient 
                    accumulation to maximize GPU utilization.
                </div>
            </div>
            <div class="card">
                <div class="card-title">Check Topology First</div>
                <div class="card-content">
                    Run <code>nvidia-smi topo -m</code> before assuming multi-GPU 
                    will help. NVLink >> PCIe >> Host Bridge.
                </div>
            </div>
        </div>

        <h2>Future Work</h2>
        <ul>
            <li>Gradient compression techniques to reduce communication overhead</li>
            <li>Asynchronous training strategies for PCIe topologies</li>
            <li>Testing with NVLink-equipped workstations</li>
            <li>Mixed precision impact on communication/computation ratio</li>
        </ul>

        <div class="tags">
            <span class="tag">Multi-GPU</span>
            <span class="tag">Hardware</span>
            <span class="tag">Distributed Training</span>
            <span class="tag">Cost Analysis</span>
            <span class="tag">PyTorch</span>
        </div>
    </article>

    <footer class="site-footer">
        <div class="footer-content">
            <span class="footer-text">© 2025 A.H. Javid · Last updated June 2025</span>
            <div class="footer-links">
                <a href="../../">Home</a>
                <a href="https://github.com/ahjavid">GitHub</a>
            </div>
        </div>
    </footer>
</body>
</html>
