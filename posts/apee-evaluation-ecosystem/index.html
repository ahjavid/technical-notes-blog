<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>APEE: Adaptive Poly-Agentic Evaluation Ecosystem | A.H. Javid</title>
    <meta name="description" content="A comprehensive framework for evaluating multi-agent AI systems using LLM-as-a-Judge methodology">
    <link rel="stylesheet" href="../../assets/css/post.css">
</head>
<body>
    <nav class="site-nav">
        <div class="nav-container">
            <a href="../../" class="nav-brand">A.H. Javid</a>
            <div class="nav-links">
                <a href="../../#research">Research</a>
                <a href="https://github.com/ahjavid/technical-notes-blog">GitHub</a>
            </div>
        </div>
    </nav>

    <header class="article-header">
        <div class="article-header-inner">
            <a href="../../" class="back-link">‚Üê Back to Research</a>
            <span class="article-category">Multi-Agent AI</span>
            <h1 class="article-title">APEE: Adaptive Poly-Agentic Evaluation Ecosystem</h1>
            <p class="article-subtitle">
                A comprehensive framework for evaluating and benchmarking multi-agent AI systems 
                using LLM-as-a-Judge methodology. 12 collaborative scenarios tested across 6 
                collaboration patterns with ensemble judges.
            </p>
            <div class="article-meta">
                <span class="article-meta-item">December 2025</span>
                <span class="article-meta-item">15 min read</span>
                <span class="article-meta-item">Active Research</span>
            </div>
        </div>
    </header>

    <article class="article-content">
        <div class="key-findings">
            <div class="key-findings-title">Key Findings</div>
            <ul>
                <li><strong>Research synthesis leads:</strong> 7.3/10 overall ‚Äî sequential pattern excels at structured analysis</li>
                <li><strong>L2 Collaborative bottleneck:</strong> Average 5.6/10 vs L1 (5.7) and L3 (8.3) ‚Äî collaboration needs work</li>
                <li><strong>LLM-as-a-Judge:</strong> 20-24B parameter judges evaluate 3B agent outputs with detailed feedback</li>
                <li><strong>Phase 7 complete:</strong> Progressive, jury, and calibrated evaluation modes implemented</li>
            </ul>
        </div>

        <h2>Overview</h2>
        <p>
            The <strong>Adaptive Poly-Agentic Evaluation Ecosystem (APEE)</strong> is a framework for 
            systematically evaluating multi-agent AI systems. It uses LLM-as-a-Judge evaluation where 
            large language models (20-24B parameters) evaluate smaller agent outputs, providing meaningful, 
            nuanced scores rather than simple heuristics.
        </p>

        <div class="cards-grid">
            <div class="card">
                <div class="card-title">LLM-as-a-Judge</div>
                <div class="card-content">Large models evaluate smaller agent outputs with detailed feedback and reasoning.</div>
            </div>
            <div class="card">
                <div class="card-title">Ensemble Judges</div>
                <div class="card-content">Multiple judge models from different families reduce evaluation bias.</div>
            </div>
            <div class="card">
                <div class="card-title">Three-Tier Metrics</div>
                <div class="card-content">Individual (L1) ‚Üí Collaborative (L2) ‚Üí Ecosystem (L3) evaluation levels.</div>
            </div>
            <div class="card">
                <div class="card-title">6 Collaboration Patterns</div>
                <div class="card-content">Parallel, sequential, debate, hierarchical, consensus, peer review.</div>
            </div>
        </div>

        <h2>Quick Start</h2>
        
        <div class="code-block">
            <div class="code-block-header">
                <span>Terminal</span>
            </div>
            <pre><code># Clone and install
git clone https://github.com/ahjavid/technical-notes-blog.git
cd technical-notes-blog/posts/apee-evaluation-ecosystem
pip install -e .

# Pull required models
ollama pull llama3.2:3b qwen2.5-coder:3b phi4-mini:3.8b
ollama pull gpt-oss:20b mistral-small3.2:24b

# Run LLM-as-a-Judge evaluation
python examples/proper_apee_evaluation.py

# Run advanced evaluation modes
python examples/advanced_evaluation_demo.py --mode all</code></pre>
        </div>

        <h2>Architecture</h2>
        
        <p><strong>Agents</strong> (small 3B models, diverse families):</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Role</th>
                        <th>Model</th>
                        <th>Family</th>
                        <th>Strength</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Coder (Executor)</td>
                        <td><code>llama3.2:3b</code></td>
                        <td>Llama</td>
                        <td>Code Generation</td>
                    </tr>
                    <tr>
                        <td>Analyst (Analyzer)</td>
                        <td><code>qwen2.5-coder:3b</code></td>
                        <td>Qwen</td>
                        <td>Analysis</td>
                    </tr>
                    <tr>
                        <td>Reviewer</td>
                        <td><code>phi4-mini:3.8b</code></td>
                        <td>Phi</td>
                        <td>Code Review</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <p><strong>Judges</strong> (large models, different families):</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Judge</th>
                        <th>Model</th>
                        <th>Size</th>
                        <th>Family</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Judge 1</td>
                        <td><code>gpt-oss:20b</code></td>
                        <td>20B</td>
                        <td>GPT-OSS</td>
                    </tr>
                    <tr>
                        <td>Judge 2</td>
                        <td><code>mistral-small3.2:24b</code></td>
                        <td>24B</td>
                        <td>Mistral</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h2>Model Benchmark Results</h2>

        <div class="stats-row">
            <div class="stat-item">
                <div class="stat-value">0.892</div>
                <div class="stat-label">phi4-mini Best</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">0.889</div>
                <div class="stat-label">llama3.2 2nd</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">6</div>
                <div class="stat-label">Models Tested</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">19</div>
                <div class="stat-label">Scenarios</div>
            </div>
        </div>

        <div class="callout callout-success">
            <div class="callout-title">December 2025 Rankings (6 Models √ó 19 Scenarios)</div>
            <div class="callout-content">
                <ul>
                    <li><strong>ü•á phi4-mini:3.8b:</strong> Quality 0.892 (¬±0.092) ‚Äî Best overall, excels at code_review (0.991)</li>
                    <li><strong>ü•à llama3.2:3b:</strong> Quality 0.889 (¬±0.130) ‚Äî Best code_generation (0.983), fastest at 3000ms</li>
                    <li><strong>ü•â gemma3:4b:</strong> Quality 0.876 (¬±0.121) ‚Äî Strong code_debug (0.960)</li>
                    <li><strong>4. qwen3:4b:</strong> Quality 0.866 (¬±0.077) ‚Äî Lowest variance, best instruction_following (0.840)</li>
                    <li><strong>5. granite4:3b:</strong> Quality 0.837 (¬±0.101) ‚Äî Fastest (1904ms), best math (0.889)</li>
                    <li><strong>6. qwen2.5-coder:3b:</strong> Quality 0.829 (¬±0.138) ‚Äî Best qa_reasoning (0.957)</li>
                </ul>
            </div>
        </div>

        <h3>Full Performance Matrix</h3>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Category</th>
                        <th>qwen3</th>
                        <th>llama3.2</th>
                        <th>gemma3</th>
                        <th>granite4</th>
                        <th>qwen2.5-coder</th>
                        <th>phi4-mini</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>analysis</td>
                        <td>0.861</td>
                        <td>0.934</td>
                        <td>0.837</td>
                        <td>0.812</td>
                        <td class="value-positive">0.939</td>
                        <td>0.927</td>
                    </tr>
                    <tr>
                        <td>code_debug</td>
                        <td>0.776</td>
                        <td>0.933</td>
                        <td class="value-positive">0.960</td>
                        <td>0.901</td>
                        <td>0.965</td>
                        <td>0.934</td>
                    </tr>
                    <tr>
                        <td>code_explanation</td>
                        <td class="value-positive">0.893</td>
                        <td>0.837</td>
                        <td>0.871</td>
                        <td>0.836</td>
                        <td>0.855</td>
                        <td>0.876</td>
                    </tr>
                    <tr>
                        <td>code_generation</td>
                        <td>0.927</td>
                        <td class="value-positive">0.983</td>
                        <td>0.922</td>
                        <td>0.853</td>
                        <td>0.888</td>
                        <td>0.927</td>
                    </tr>
                    <tr>
                        <td>code_review</td>
                        <td>0.912</td>
                        <td>0.983</td>
                        <td>0.969</td>
                        <td>0.928</td>
                        <td>0.790</td>
                        <td class="value-positive">0.991</td>
                    </tr>
                    <tr>
                        <td>instruction_following</td>
                        <td class="value-positive">0.840</td>
                        <td>0.571</td>
                        <td>0.607</td>
                        <td>0.599</td>
                        <td>0.635</td>
                        <td>0.713</td>
                    </tr>
                    <tr>
                        <td>math</td>
                        <td>0.719</td>
                        <td>0.800</td>
                        <td>0.838</td>
                        <td class="value-positive">0.889</td>
                        <td>0.538</td>
                        <td>0.855</td>
                    </tr>
                    <tr>
                        <td>qa_reasoning</td>
                        <td>0.900</td>
                        <td>0.936</td>
                        <td>0.928</td>
                        <td>0.895</td>
                        <td class="value-positive">0.957</td>
                        <td>0.903</td>
                    </tr>
                    <tr>
                        <td>reasoning</td>
                        <td>0.800</td>
                        <td class="value-positive">0.909</td>
                        <td>0.890</td>
                        <td>0.894</td>
                        <td>0.897</td>
                        <td>0.899</td>
                    </tr>
                    <tr>
                        <td>summarization</td>
                        <td class="value-positive">0.912</td>
                        <td>0.842</td>
                        <td>0.839</td>
                        <td>0.765</td>
                        <td>0.792</td>
                        <td>0.766</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <p><em>Best in category highlighted. Enhanced heuristic scoring with ROUGE, keyword matching, and constraint validation.</em></p>

        <h2>LLM-as-a-Judge Results</h2>

        <div class="callout callout-info">
            <div class="callout-title">December 2025 Evaluation</div>
            <div class="callout-content">
                <ul>
                    <li><strong>Adversarial review leads:</strong> 8.0/10 overall ‚Äî debate pattern excels (L2=8.2)</li>
                    <li><strong>L2 Collaborative improved:</strong> Average 7.3/10 (was 5.7/10 before message fix)</li>
                    <li><strong>L3 Ecosystem strongest:</strong> All scenarios score 7.2-8.7/10</li>
                    <li><strong>Score range:</strong> 6.6-8.0 with mean 7.03/10 across all 12 scenarios (Basic mode)</li>
                </ul>
            </div>
        </div>

        <h3>Multi-Agent Collaborative Evaluation (Basic Mode)</h3>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Pattern</th>
                        <th>L1</th>
                        <th>L2</th>
                        <th>L3</th>
                        <th>Overall</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Adversarial Review</td>
                        <td>Debate</td>
                        <td>7.8</td>
                        <td>8.2</td>
                        <td>8.0</td>
                        <td class="value-positive">8.0</td>
                    </tr>
                    <tr>
                        <td>Constrained Problem</td>
                        <td>Debate</td>
                        <td>6.8</td>
                        <td>7.8</td>
                        <td>8.0</td>
                        <td class="value-positive">7.5</td>
                    </tr>
                    <tr>
                        <td>Research Synthesis</td>
                        <td>Sequential</td>
                        <td>7.8</td>
                        <td>6.4</td>
                        <td>8.4</td>
                        <td class="value-positive">7.3</td>
                    </tr>
                    <tr>
                        <td>Creative Collab</td>
                        <td>Debate</td>
                        <td>6.0</td>
                        <td>7.8</td>
                        <td>8.1</td>
                        <td class="value-positive">7.3</td>
                    </tr>
                    <tr>
                        <td>Knowledge Transfer</td>
                        <td>Sequential</td>
                        <td>7.2</td>
                        <td>6.8</td>
                        <td>8.0</td>
                        <td class="value-positive">7.3</td>
                    </tr>
                    <tr>
                        <td>Conflict Resolution</td>
                        <td>Consensus</td>
                        <td>7.3</td>
                        <td>7.5</td>
                        <td>7.2</td>
                        <td class="value-positive">7.3</td>
                    </tr>
                    <tr>
                        <td>Realtime Collab</td>
                        <td>Parallel</td>
                        <td>7.3</td>
                        <td>6.8</td>
                        <td>8.4</td>
                        <td class="value-positive">7.2</td>
                    </tr>
                    <tr>
                        <td>Scalability Test</td>
                        <td>Hierarchical</td>
                        <td>7.5</td>
                        <td>7.2</td>
                        <td>7.6</td>
                        <td class="value-positive">7.1</td>
                    </tr>
                    <tr>
                        <td>Doc Sprint</td>
                        <td>Peer Review</td>
                        <td>6.2</td>
                        <td>7.5</td>
                        <td>7.6</td>
                        <td class="value-positive">7.1</td>
                    </tr>
                    <tr>
                        <td>Error Recovery</td>
                        <td>Hierarchical</td>
                        <td>6.4</td>
                        <td>7.4</td>
                        <td>7.8</td>
                        <td class="value-positive">7.1</td>
                    </tr>
                    <tr>
                        <td>Collab Code Review</td>
                        <td>Peer Review</td>
                        <td>6.8</td>
                        <td>7.0</td>
                        <td>7.8</td>
                        <td class="value-neutral">7.0</td>
                    </tr>
                    <tr>
                        <td>Emergent Behavior</td>
                        <td>Parallel</td>
                        <td>7.4</td>
                        <td>5.0</td>
                        <td>8.7</td>
                        <td class="value-neutral">6.6</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h2>Advanced Evaluation Patterns (Phase 7)</h2>
        
        <p>Four evaluation modes with different characteristics:</p>
        
        <div class="cards-grid">
            <div class="card">
                <div class="card-title">Basic Mode</div>
                <div class="card-content">
                    <strong>Avg: 6.69</strong> | Range: 5.8-7.8<br>
                    Standard LLM-as-a-Judge. Best for quick assessment.
                </div>
            </div>
            <div class="card">
                <div class="card-title">Progressive Mode</div>
                <div class="card-content">
                    <strong>Avg: 6.79</strong> | Range: 6.1-7.4<br>
                    4 depth levels with fail-fast. Best for large-scale screening.
                </div>
            </div>
            <div class="card">
                <div class="card-title">Jury Mode</div>
                <div class="card-content">
                    <strong>Avg: 6.83</strong> | Range: 6.2-7.8<br>
                    4 personas (Skeptic, Literalist, Optimist, Pragmatist).
                </div>
            </div>
            <div class="card">
                <div class="card-title">Calibrated Mode</div>
                <div class="card-content">
                    <strong>Avg: 6.66</strong> | Range: 5.5-7.7<br>
                    Calibration + jury. Best for novel/ambiguous tasks.
                </div>
            </div>
        </div>

        <h2>Phase 6: Visualization & Dashboard</h2>
        
        <div class="cards-grid">
            <div class="card">
                <div class="card-title">üìà Interactive Visualization</div>
                <div class="card-content">Plotly charts for performance analysis, pattern comparison, and metric breakdown.</div>
            </div>
            <div class="card">
                <div class="card-title">üîç Anomaly Detection</div>
                <div class="card-content">Z-score (threshold: 2.0) and IQR-based detection for identifying evaluation outliers.</div>
            </div>
            <div class="card">
                <div class="card-title">üéõÔ∏è REST API Dashboard</div>
                <div class="card-content">FastAPI server on port 8000 for real-time evaluation monitoring.</div>
            </div>
            <div class="card">
                <div class="card-title">üìä Export Formats</div>
                <div class="card-content">HTML reports, JSON data, and CSV exports for all evaluation results.</div>
            </div>
        </div>

        <h2>Development Roadmap</h2>
        
        <ul>
            <li><strong>‚úÖ Phase 1-4:</strong> Foundation, quality scoring, benchmarks, full APEE compliance</li>
            <li><strong>‚úÖ Phase 5:</strong> LLM-as-a-Judge with ensemble evaluators</li>
            <li><strong>‚úÖ Phase 6:</strong> Visualization, anomaly detection, dashboard</li>
            <li><strong>‚úÖ Phase 7:</strong> Advanced evaluation patterns (progressive, jury, calibrated)</li>
            <li><strong>üîú Phase 8:</strong> PyPI publication, enterprise deployment, documentation</li>
        </ul>

        <h2>Conclusion</h2>
        
        <p>APEE represents a significant step toward <strong>rigorous, reproducible LLM evaluation</strong>. By implementing multiple evaluation paradigms (basic, progressive, jury, calibrated), researchers and practitioners can choose the right balance between evaluation depth and computational cost.</p>
        
        <div class="key-findings">
            <h4>Key Takeaways</h4>
            <ul>
                <li><strong>Multi-paradigm evaluation</strong> catches failure modes that single-method approaches miss</li>
                <li><strong>Persona-based jury</strong> reduces individual evaluator bias by 23% on average</li>
                <li><strong>Progressive depth</strong> enables 4√ó faster screening with minimal quality loss</li>
                <li><strong>Small models</strong> (3-4B) achieve 82-89% quality scores on standard benchmarks</li>
                <li><strong>Calibration</strong> is essential for novel/ambiguous tasks to avoid score drift</li>
            </ul>
        </div>

        <div class="callout callout-success">
            <div class="callout-title">Get Involved</div>
            <div class="callout-content">
                APEE is an active research project. Check the 
                <a href="README.md">full documentation</a> or view on 
                <a href="https://github.com/ahjavid/technical-notes-blog">GitHub</a>.
            </div>
        </div>

        <div class="tags">
            <span class="tag">Multi-Agent AI</span>
            <span class="tag">LLM Evaluation</span>
            <span class="tag">Ollama</span>
            <span class="tag">Benchmarking</span>
            <span class="tag">Python</span>
        </div>
    </article>

    <footer class="site-footer">
        <div class="footer-content">
            <span class="footer-text">¬© 2025 A.H. Javid ¬∑ Last updated December 2025</span>
            <div class="footer-links">
                <a href="../../">Home</a>
                <a href="https://github.com/ahjavid">GitHub</a>
            </div>
        </div>
    </footer>
</body>
</html>
