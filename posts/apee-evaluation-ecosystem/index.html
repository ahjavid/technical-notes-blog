<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>APEE: Adaptive Poly-Agentic Evaluation Ecosystem | A.H. Javid</title>
    <meta name="description" content="A comprehensive framework for evaluating multi-agent AI systems using LLM-as-a-Judge methodology">
    <link rel="stylesheet" href="../../assets/css/post.css">
</head>
<body>
    <nav class="site-nav">
        <div class="nav-container">
            <a href="../../" class="nav-brand">A.H. Javid</a>
            <div class="nav-links">
                <a href="../../#research">Research</a>
                <a href="https://github.com/ahjavid/technical-notes-blog">GitHub</a>
            </div>
        </div>
    </nav>

    <header class="article-header">
        <div class="article-header-inner">
            <a href="../../" class="back-link">‚Üê Back to Research</a>
            <span class="article-category">Multi-Agent AI</span>
            <h1 class="article-title">APEE: Adaptive Poly-Agentic Evaluation Ecosystem</h1>
            <p class="article-subtitle">
                A comprehensive framework for evaluating and benchmarking multi-agent AI systems 
                using LLM-as-a-Judge methodology. 12 collaborative scenarios tested across 6 
                collaboration patterns with ensemble judges.
            </p>
            <div class="article-meta">
                <span class="article-meta-item">December 2025</span>
                <span class="article-meta-item">15 min read</span>
                <span class="article-meta-item">Active Research</span>
            </div>
        </div>
    </header>

    <article class="article-content">
        <div class="key-findings">
            <div class="key-findings-title">Key Findings</div>
            <ul>
                <li><strong>Research synthesis leads:</strong> 7.3/10 overall ‚Äî sequential pattern excels at structured analysis</li>
                <li><strong>L2 Collaborative bottleneck:</strong> Average 5.6/10 vs L1 (5.7) and L3 (8.3) ‚Äî collaboration needs work</li>
                <li><strong>LLM-as-a-Judge:</strong> 20-24B parameter judges evaluate 3B agent outputs with detailed feedback</li>
                <li><strong>Phase 7 complete:</strong> Progressive, jury, and calibrated evaluation modes implemented</li>
            </ul>
        </div>

        <h2>Overview</h2>
        <p>
            The <strong>Adaptive Poly-Agentic Evaluation Ecosystem (APEE)</strong> is a framework for 
            systematically evaluating multi-agent AI systems. It uses LLM-as-a-Judge evaluation where 
            large language models (20-24B parameters) evaluate smaller agent outputs, providing meaningful, 
            nuanced scores rather than simple heuristics.
        </p>

        <div class="cards-grid">
            <div class="card">
                <div class="card-title">LLM-as-a-Judge</div>
                <div class="card-content">Large models evaluate smaller agent outputs with detailed feedback and reasoning.</div>
            </div>
            <div class="card">
                <div class="card-title">Ensemble Judges</div>
                <div class="card-content">Multiple judge models from different families reduce evaluation bias.</div>
            </div>
            <div class="card">
                <div class="card-title">Three-Tier Metrics</div>
                <div class="card-content">Individual (L1) ‚Üí Collaborative (L2) ‚Üí Ecosystem (L3) evaluation levels.</div>
            </div>
            <div class="card">
                <div class="card-title">6 Collaboration Patterns</div>
                <div class="card-content">Parallel, sequential, debate, hierarchical, consensus, peer review.</div>
            </div>
        </div>

        <h2>Quick Start</h2>
        
        <div class="code-block">
            <div class="code-block-header">
                <span>Terminal</span>
            </div>
            <pre><code># Clone and install
git clone https://github.com/ahjavid/technical-notes-blog.git
cd technical-notes-blog/posts/apee-evaluation-ecosystem
pip install -e .

# Pull required models
ollama pull llama3.2:3b qwen2.5-coder:3b granite4:3b phi4-mini:3.8b
ollama pull gpt-oss:20b mistral-small3.2:24b

# Run LLM-as-a-Judge evaluation
python examples/proper_apee_evaluation.py

# Run advanced evaluation modes
python examples/advanced_evaluation_demo.py --mode all</code></pre>
        </div>

        <h2>Architecture</h2>
        
        <p><strong>Agents</strong> (small 3B models, diverse families):</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Role</th>
                        <th>Model</th>
                        <th>Family</th>
                        <th>Strength</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Coder (Executor)</td>
                        <td><code>llama3.2:3b</code></td>
                        <td>Llama</td>
                        <td>Code Generation</td>
                    </tr>
                    <tr>
                        <td>Analyst (Analyzer)</td>
                        <td><code>qwen2.5-coder:3b</code></td>
                        <td>Qwen</td>
                        <td>Analysis</td>
                    </tr>
                    <tr>
                        <td>Reviewer</td>
                        <td><code>granite4:3b</code></td>
                        <td>Granite</td>
                        <td>Code Review</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <p><strong>Judges</strong> (large models, different families):</p>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Judge</th>
                        <th>Model</th>
                        <th>Size</th>
                        <th>Family</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Judge 1</td>
                        <td><code>gpt-oss:20b</code></td>
                        <td>20B</td>
                        <td>GPT-OSS</td>
                    </tr>
                    <tr>
                        <td>Judge 2</td>
                        <td><code>mistral-small3.2:24b</code></td>
                        <td>24B</td>
                        <td>Mistral</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h2>LLM-as-a-Judge Results</h2>

        <div class="callout callout-info">
            <div class="callout-title">December 2025 Evaluation</div>
            <div class="callout-content">
                <ul>
                    <li><strong>Conflict resolution leads:</strong> 7.8/10 overall ‚Äî consensus pattern excels</li>
                    <li><strong>L2 Collaborative is the bottleneck:</strong> Average 5.7/10 vs L1 (6.9/10) and L3 (8.2/10)</li>
                    <li><strong>L3 Ecosystem strongest:</strong> All scenarios score 7.4-8.8/10</li>
                    <li><strong>Score range:</strong> 5.8-7.8 with mean 6.7/10 across all 12 scenarios</li>
                </ul>
            </div>
        </div>

        <h3>Multi-Agent Collaborative Evaluation</h3>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Pattern</th>
                        <th>L1</th>
                        <th>L2</th>
                        <th>L3</th>
                        <th>Overall</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Conflict Resolution</td>
                        <td>Consensus</td>
                        <td>7.8</td>
                        <td>7.5</td>
                        <td>8.3</td>
                        <td class="value-positive">7.8</td>
                    </tr>
                    <tr>
                        <td>Research Synthesis</td>
                        <td>Sequential</td>
                        <td>7.5</td>
                        <td>6.2</td>
                        <td>8.6</td>
                        <td class="value-positive">7.2</td>
                    </tr>
                    <tr>
                        <td>Constrained Problem</td>
                        <td>Debate</td>
                        <td>7.1</td>
                        <td>6.5</td>
                        <td>8.6</td>
                        <td class="value-positive">7.2</td>
                    </tr>
                    <tr>
                        <td>Knowledge Transfer</td>
                        <td>Sequential</td>
                        <td>7.1</td>
                        <td>6.1</td>
                        <td>8.7</td>
                        <td class="value-positive">7.1</td>
                    </tr>
                    <tr>
                        <td>Adversarial Review</td>
                        <td>Debate</td>
                        <td>7.5</td>
                        <td>6.2</td>
                        <td>8.2</td>
                        <td class="value-positive">7.1</td>
                    </tr>
                    <tr>
                        <td>Error Recovery</td>
                        <td>Hierarchical</td>
                        <td>5.7</td>
                        <td>6.8</td>
                        <td>8.6</td>
                        <td class="value-neutral">6.9</td>
                    </tr>
                    <tr>
                        <td>Creative Collab</td>
                        <td>Debate</td>
                        <td>6.6</td>
                        <td>6.0</td>
                        <td>8.5</td>
                        <td class="value-neutral">6.8</td>
                    </tr>
                    <tr>
                        <td>Realtime Collab</td>
                        <td>Parallel</td>
                        <td>7.0</td>
                        <td>5.0</td>
                        <td>8.8</td>
                        <td class="value-neutral">6.5</td>
                    </tr>
                    <tr>
                        <td>Doc Sprint</td>
                        <td>Peer Review</td>
                        <td>6.6</td>
                        <td>5.8</td>
                        <td>7.6</td>
                        <td class="value-neutral">6.5</td>
                    </tr>
                    <tr>
                        <td>Collab Code Review</td>
                        <td>Peer Review</td>
                        <td>6.8</td>
                        <td>5.5</td>
                        <td>7.4</td>
                        <td class="value-neutral">6.4</td>
                    </tr>
                    <tr>
                        <td>Emergent Behavior</td>
                        <td>Parallel</td>
                        <td>7.2</td>
                        <td>3.8</td>
                        <td>8.4</td>
                        <td class="value-neutral">6.0</td>
                    </tr>
                    <tr>
                        <td>Scalability Test</td>
                        <td>Hierarchical</td>
                        <td>6.7</td>
                        <td>4.0</td>
                        <td>8.0</td>
                        <td class="value-negative">5.8</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h2>Model Benchmark Results</h2>

        <div class="stats-row">
            <div class="stat-item">
                <div class="stat-value">0.892</div>
                <div class="stat-label">phi4-mini Best</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">0.889</div>
                <div class="stat-label">llama3.2 2nd</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">6</div>
                <div class="stat-label">Models Tested</div>
            </div>
            <div class="stat-item">
                <div class="stat-value">19</div>
                <div class="stat-label">Scenarios</div>
            </div>
        </div>

        <h3>Performance by Task Category</h3>
        
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Category</th>
                        <th>qwen3:4b</th>
                        <th>llama3.2:3b</th>
                        <th>gemma3:4b</th>
                        <th>phi4-mini:3.8b</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>code_generation</td>
                        <td>0.927</td>
                        <td class="value-positive">0.983</td>
                        <td>0.922</td>
                        <td>0.927</td>
                    </tr>
                    <tr>
                        <td>code_review</td>
                        <td>0.912</td>
                        <td>0.983</td>
                        <td>0.969</td>
                        <td class="value-positive">0.991</td>
                    </tr>
                    <tr>
                        <td>code_debug</td>
                        <td>0.776</td>
                        <td>0.933</td>
                        <td class="value-positive">0.960</td>
                        <td>0.934</td>
                    </tr>
                    <tr>
                        <td>qa_reasoning</td>
                        <td>0.900</td>
                        <td>0.936</td>
                        <td>0.928</td>
                        <td>0.903</td>
                    </tr>
                    <tr>
                        <td>instruction_following</td>
                        <td class="value-positive">0.840</td>
                        <td>0.571</td>
                        <td>0.607</td>
                        <td>0.713</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <h2>Advanced Evaluation Patterns (Phase 7)</h2>
        
        <p>Four evaluation modes with different characteristics:</p>
        
        <div class="cards-grid">
            <div class="card">
                <div class="card-title">Basic Mode</div>
                <div class="card-content">
                    <strong>Avg: 6.69</strong> | Range: 5.8-7.8<br>
                    Standard LLM-as-a-Judge. Best for quick assessment.
                </div>
            </div>
            <div class="card">
                <div class="card-title">Progressive Mode</div>
                <div class="card-content">
                    <strong>Avg: 6.79</strong> | Range: 6.1-7.4<br>
                    4 depth levels with fail-fast. Best for large-scale screening.
                </div>
            </div>
            <div class="card">
                <div class="card-title">Jury Mode</div>
                <div class="card-content">
                    <strong>Avg: 6.83</strong> | Range: 6.2-7.8<br>
                    4 personas (Skeptic, Literalist, Optimist, Pragmatist).
                </div>
            </div>
            <div class="card">
                <div class="card-title">Calibrated Mode</div>
                <div class="card-content">
                    <strong>Avg: 6.66</strong> | Range: 5.5-7.7<br>
                    Calibration + jury. Best for novel/ambiguous tasks.
                </div>
            </div>
        </div>

        <h2>Development Roadmap</h2>
        
        <ul>
            <li><strong>‚úÖ Phase 1-4:</strong> Foundation, quality scoring, benchmarks, full APEE compliance</li>
            <li><strong>‚úÖ Phase 5:</strong> LLM-as-a-Judge with ensemble evaluators</li>
            <li><strong>‚úÖ Phase 6:</strong> Visualization, anomaly detection, dashboard</li>
            <li><strong>‚úÖ Phase 7:</strong> Advanced evaluation patterns (progressive, jury, calibrated)</li>
            <li><strong>üîú Phase 8:</strong> PyPI publication, enterprise deployment, documentation</li>
        </ul>

        <div class="callout callout-success">
            <div class="callout-title">Get Involved</div>
            <div class="callout-content">
                APEE is an active research project. Check the 
                <a href="README.md">full documentation</a> or view on 
                <a href="https://github.com/ahjavid/technical-notes-blog">GitHub</a>.
            </div>
        </div>

        <div class="tags">
            <span class="tag">Multi-Agent AI</span>
            <span class="tag">LLM Evaluation</span>
            <span class="tag">Ollama</span>
            <span class="tag">Benchmarking</span>
            <span class="tag">Python</span>
        </div>
    </article>

    <footer class="site-footer">
        <div class="footer-content">
            <span class="footer-text">¬© 2025 A.H. Javid ¬∑ Last updated December 2025</span>
            <div class="footer-links">
                <a href="../../">Home</a>
                <a href="https://github.com/ahjavid">GitHub</a>
            </div>
        </div>
    </footer>
</body>
</html>
